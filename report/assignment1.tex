\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{packages}
\renewcommand{\b}[1]{\boldsymbol{#1}}
\newcommand{\pfrac}[2]{\frac{\partial #1}{\partial #2}}
\title{Deep Learning Assignment 1}

\author{%
  Jochem Loedeman \\
  12995282
}

\begin{document}

\maketitle


\section{MLP backprop and NumPy Implementation}
\subsection{Evaluating the Gradients}
\subsubsection*{Question 1.1}
\begin{enumerate}[label=(\alph*)]
	\item 
	Given that
	$$
	\b Y = \b X \b W^T + \b B,
	$$ we deduce that
	$$
	Y_{mn} = \sum_p X_{mp}W_{np} + B_{mn}.
	$$ We will now find the required derivatives.
	\begin{enumerate}[label=(\roman*)]
	\item 
	$$
	\begin{aligned}
	\pfrac{L}{W_{ij}} &= \sum_{m, n}\pfrac{L}{Y_{mn}}\pfrac{Y_{mn}}{W_{ij}}.
	\end{aligned}
	$$ But
	$$
	\begin{aligned}
	\pfrac{Y_{mn}}{W_{ij}} &= \sum_pX_{mp}\pfrac{W_{np}}{W_{ij}} = \sum_pX_{mp}\delta_{ni}\delta_{pj} = X_{mj}\delta_{ni},
	\end{aligned}
	$$ and hence
	$$
	\begin{aligned}
	\pfrac{L}{W_{ij}} &= \sum_{m, n}\pfrac{L}{Y_{mn}}X_{mj}\delta_{ni} = \sum_m \pfrac{L}{Y_{mi}}X_{mj}.
	\end{aligned}
	$$ In matrix-vector notation, this is equivalent to
	$$
	\pfrac{L}{\b W} = \left(\pfrac{L}{\b Y}\right)^T\b X
	$$
	\item
	$$
	\pfrac{L}{b_i} = \sum_{m, n}\pfrac{L}{Y_{mn}}\pfrac{Y_{mn}}{b_i}
	$$
	But
	$$
	\pfrac{Y_{mn}}{b_i} = \pfrac{B_{mn}}{b_i} = \delta_{ni},
	$$ Since $B_{mn} = b_n$ for all $m = 1,\dots, S$. Therefore,
	$$
	\pfrac{L}{b_i} = \sum_{m, n}\pfrac{L}{Y_{mn}}\delta_{ni} = \sum_{m}\pfrac{L}{Y_{mi}}.
	$$ In matrix-vector notation, this is equivalent to
	$$
	\pfrac{L}{\b b} = \b 1 \pfrac{L}{\b Y}
	$$ where $\b 1$ is the $1 \times S$ ones-vector.
	\item 
	$$
	\pfrac{L}{X_{ij}} = \sum_{m, n}\pfrac{L}{Y_{mn}}\pfrac{Y_{mn}}{X_{ij}}.
	$$ But
	$$
	\pfrac{Y_{mn}}{X_{ij}} = \sum_p\pfrac{X_{mp}}{X_{ij}}W_{np} = \sum_p\delta_{mi}\delta_{pj}W_{np} = \delta_{mi}W_{nj},
	$$
	so
	$$
	\pfrac{L}{X_{ij}} = \sum_{m, n}\pfrac{L}{Y_{mn}}\delta_{mi}W_{nj} = \sum_n \pfrac{L}{Y_{in}}W_{nj}.
	$$ In matrix-vector notation, this is equivalent to
	$$
	\pfrac{L}{\b X} = \pfrac{L}{\b Y}\b W
	$$
\end{enumerate}
	\item Like before, we have
	$$
	\pfrac{L}{X_{ij}} = \sum_{m, n}\pfrac{L}{Y_{mn}}\pfrac{Y_{mn}}{X_{ij}}
	$$ But
	$$
	\pfrac{Y_{mn}}{X_{ij}} = \pfrac{h(X_{mn})}{X_{ij}} = h'(X_{mn})\delta_{mi}\delta_{nj},
	$$ and therefore,
	$$
	\pfrac{L}{X_{ij}} = \sum_{m, n}\pfrac{L}{Y_{mn}}h'(X_{mn})\delta_{mi}\delta_{nj} = \pfrac{L}{Y_{ij}}h'(X_{ij}).
	$$ Or, in matrix-vector notation:
	$$\pfrac{L}{\b X} = \pfrac{L}{Y} \circ h'(\b X)$$
	\item
	\begin{enumerate}[label=(\roman*)]
		\item
		$$
		 \pfrac{L}{X_{ij}} = \sum_{m, n}\pfrac{L}{Y_{mn}}\pfrac{Y_{mn}}{X_{ij}}.
		$$ The derivative of the softmax is
		$$
		\begin{aligned}
		\pfrac{Y_{mn}}{X_{ij}} &= \pfrac{}{X_{ij}} \frac{e^{X_{mn}}}{\sum_ke^{X_{mk}}} \\ &= \frac{\sum_ke^{X_{mk}} \cdot e^{X_{mn}}\delta_{mi}\delta_{nj} - e^{X_{mn}}\cdot\sum_ke^{X_{mk}}\delta_{mi}\delta_{kj}}{\left(\sum_ke^{X_{mk}}\right)^2} \\ &= \frac{\sum_ke^{X_{mk}} \cdot e^{X_{mn}}\delta_{mi}\delta_{nj} - e^{X_{mn}}\cdot e^{X_{mj}}\delta_{mi}}{\left(\sum_ke^{X_{mk}}\right)^2}\\
		&= Y_{mn}\delta_{mi}\delta_{nj} - Y_{mn}Y_{mj}\delta_{mi} \\ &= Y_{mn}\delta_{mi}\left(\delta_{nj} - Y_{mj}\right)
		\end{aligned}
		$$ Notice that when $m = i$ (i.e. when the same data point is considered), the derivative reduces to the usual softmax derivative for rank-1 tensors.
		
		We get
		$$
		\begin{aligned}
		\pfrac{L}{X_{ij}} &= \sum_{m, n}\pfrac{L}{Y_{mn}}Y_{mn}\delta_{mi}\left(\delta_{nj} - Y_{mj}\right) \\ &= \sum_n\pfrac{L}{Y_{in}}Y_{in}\left(\delta_{nj} - Y_{mj}\right)
		\end{aligned}
		$$
		\item 
		$$
		\begin{aligned}
		\pfrac{L}{X_{ij}} &= -\frac{1}{S}\sum_{m, n}T_{mn}\pfrac{\log X_{mn}}{X_{ij} }\\ &= -\frac{1}{S}\sum_{m, n}\frac{T_{mn}}{X_{mn}}\pfrac{X_{mn}}{X_{ij}} \\ &= -\frac{1}{S}\sum_{m, n}\frac{T_{mn}}{X_{mn}}\delta_{mi}\delta_{nj} \\ &= -\frac{1}{S}\frac{T_{ij}}{X_{ij}}
		\end{aligned}
		$$ The equation can be vectorized by using the elementwise division operator, known as the Hadamard division.
		$$
		\pfrac{L}{\b X} = -\frac{1}{S} \; \b T \oslash \b X
		$$
	\end{enumerate}
\end{enumerate}


\end{document}
